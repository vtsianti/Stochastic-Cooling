{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "betPU, alfPU       =  1.3, -0.9\n",
    "betK, alfK         =  0.8,  0.6\n",
    "gamPU, gamK        =  (1 + alfPU**2)/betPU, (1 + alfK**2)/betK\n",
    "muPUK, muKPU       =  1.51, 2*np.pi*1.34 - 1.51\n",
    "total_angle        =  2*np.pi*1.34\n",
    "etaPUK, etaKPU     =  -0.01, -0.01     #eta PUK must be reletevely small, so they dont change slices so fast... 0.02 breaks the system ... It is the limit case!!!! very sensitive on etaPUK --> 2 much unwanted mixing, also etaKPU cant go very low(limit case 0.3) because then the mixing is bad\n",
    "sigdelt            = .0001            # rms momentum spread in unit\n",
    "Th, Trev           =  .1e-6, 5.0e-6, # half width of simulation window and revolution period in s\n",
    "Tovlap, NSamp, add = 4*sigdelt*abs(etaPUK + etaKPU)*Trev, 800, '2000'\n",
    "    #gain               =  0.00018  #as long as I reduce the eta, i have to reduce gains as well... see book\n",
    "    #gainL              =  3e-8\n",
    "over_sample_factor=10\n",
    "DTSamp   = Th/NSamp\n",
    "NSampext = math.ceil( Tovlap/DTSamp )\n",
    "inds = np.arange( (-NSampext)*over_sample_factor , (NSamp+1+NSampext + 1)*over_sample_factor )\n",
    "t_bins = inds*DTSamp/over_sample_factor -Th/2\n",
    "wlL, wrL = -1.6e-9, 1.6e-9\n",
    "wlH, wrH = -1.6e-9, 1.6e-9\n",
    "\n",
    "rng = np.random.default_rng(2275405 )\n",
    "\n",
    "#time delay cooling here!~!!!!!1\n",
    "#this approach -> bin centers main = only bin centers main\n",
    "#also cut the time offset ->Th+Dtsamp+Tovlap\n",
    "\n",
    "#datin  = time.time()  # record start time - to estimate run duration at end\n",
    "\n",
    "def running_average(x, y, bins):\n",
    "     \"\"\"\n",
    "     Compute the running average of y over fixed intervals in x efficiently using NumPy.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): Array of x-values (sorted or unsorted).\n",
    "    y (array-like): Array of y-values corresponding to x.\n",
    "    interval_width (float): The width of the intervals.\n",
    "\n",
    "    Returns:\n",
    "    bin_centers (numpy.ndarray): The centers of the bins.\n",
    "    averages (numpy.ndarray): The average y-values for each bin.\n",
    "     \"\"\"\n",
    "    # Ensure inputs are NumPy arrays\n",
    "     x = np.array(x)\n",
    "     y = np.array(y)\n",
    "\n",
    "    # Bin the data using NumPy digitize\n",
    "     bin_indices = np.digitize(x, bins) - 1  # Get zero-based bin indices\n",
    "\n",
    "    # keep only elements within bins\n",
    "     valid_input_indices = (bin_indices > 0) & (bin_indices < len(bins) - 1)\n",
    "\n",
    "    # Preallocate array for averages\n",
    "     averages = np.zeros(len(bins) - 1, dtype=float)\n",
    "    \n",
    "    # Use NumPy's bincount for fast bin-based summation and counts\n",
    "     bin_sums   = np.bincount(bin_indices[valid_input_indices], minlength=len(bins) - 1, weights=y[valid_input_indices])\n",
    "     bin_counts = np.bincount(bin_indices[valid_input_indices], minlength=len(bins) - 1)\n",
    "    \n",
    "    # Avoid division by zero: compute averages where bin_counts > 0\n",
    "     non_empty_bins = bin_counts > 0\n",
    "     averages[non_empty_bins] = bin_sums[non_empty_bins] / bin_counts[non_empty_bins]\n",
    "\n",
    "    # Compute bin centers\n",
    "     bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "     return bin_centers, averages, bin_counts, bin_sums\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Routine to do do the exchange of .. ended up by regrouping from scratch \n",
    "def PartsExchange( ptsEarly, ptsLate,Th ):\n",
    "     ptsEarlyNew, ptsLateNew = [], []\n",
    "     for x, xp, delt, Tau in ptsEarly:\n",
    "        if Tau < Th/2:  # should remain in \"early group\n",
    "            ptsEarlyNew += [ [x, xp, delt, Tau] ]\n",
    "        else:\n",
    "            ptsLateNew += [ [x, xp, delt, Tau - Th] ]\n",
    "     for x, xp, delt, Tau in ptsLate:\n",
    "        if Tau > -Th/2:\n",
    "            ptsLateNew += [ [x, xp, delt, Tau] ]\n",
    "        else:\n",
    "            ptsEarlyNew += [ [x, xp, delt, Tau + Th] ]\n",
    "     return ptsEarlyNew, ptsLateNew\n",
    "\n",
    "\n",
    "def respH( Dt ):\n",
    "   # if Dt < 1.*wlH or Dt > 1.*wrH:\n",
    "    #    print( f' ===> function respH called with Dt ={1e6*Dt:8.4f}' )\n",
    "      return (1 - 27e18*Dt**2)*(1 - 3.e18*Dt**2)*(1 - 1.15e18*Dt**2)*(1 - (.625e9*Dt)**2)**7\n",
    "#   return (1 - 23e16*Dt**2)*(1 - 3.e16*Dt**2)*(1 - 1.15e16*Dt**2)*(1 - (.625e8*Dt)**2)**7\n",
    "#   return (1 - 25e-12*Dt^2)*(1 - 3e-12*Dt^2)*(1 - 1.15e-12*Dt^2)*(1 - (Dt/1.5)^2)^6\n",
    "\n",
    "\n",
    "def respL( Dt ):\n",
    "   # if Dt < 1.*wlL or Dt > 1.*wrL:\n",
    "    #    print( f' ===> function respL called with Dt ={1e6*Dt:8.4f}' )\n",
    "      return  (63e9*Dt)*(1 - 6.5e18*Dt**2)*(1 - 1.55e18*Dt**2)*(1 - (.625e9*Dt)**2)**8\n",
    "\n",
    "def TimeTraces(ptsEarly, ptsMain, ptsLate, bins,Th):\n",
    "     \"\"\"\n",
    "    Generate time traces without applying the `Th / 2` offset internally.\n",
    "     \"\"\"\n",
    "    \n",
    "     ptsEarly = np.array(ptsEarly)\n",
    "     ptsMain = np.array(ptsMain)\n",
    "     ptsLate = np.array(ptsLate)\n",
    "     # Compute histograms of beam distribution\n",
    "     bin_centers_early, averages_early, count_early, sum_early = running_average(\n",
    "        ptsEarly[:, 3]-Th, ptsEarly[:, 0], bins\n",
    "      )\n",
    "     bin_centers_main, averages_main, count_main, sum_main = running_average(\n",
    "        ptsMain[:, 3], ptsMain[:, 0], bins\n",
    "      )\n",
    "     bin_centers_late, averages_late, count_late, sum_late = running_average(\n",
    "        ptsLate[:, 3]+Th, ptsLate[:, 0], bins\n",
    "      )\n",
    "\n",
    "    # Compute time traces\n",
    "     TraceH_early = np.convolve(respH_new_once, sum_early, 'same')\n",
    "     TraceH_main = np.convolve(respH_new_once, sum_main, 'same')\n",
    "     TraceH_late = np.convolve(respH_new_once, sum_late, 'same')\n",
    "    \n",
    "     TraceL_early = np.convolve(respL_new_once, count_early, 'same')\n",
    "     TraceL_main = np.convolve(respL_new_once, count_main, 'same')\n",
    "     TraceL_late = np.convolve(respL_new_once, count_late, 'same')\n",
    "\n",
    "     TraceH = TraceH_early + TraceH_main + TraceH_late\n",
    "     TraceL = TraceL_early + TraceL_main + TraceL_late #i am multiplying with this factor in order to amplify the signal!!!!!\n",
    "    \n",
    "    \n",
    "    \n",
    "     return bin_centers_main, TraceH, TraceL\n",
    "def kickH(pts, signalH,gain):\n",
    "      corrected_pts = []\n",
    "      for x, xp, delt, Tau in pts:\n",
    "        # Interpolate signalH at the particle's Tau\n",
    "        signal_value = signalH(Tau)\n",
    "        # Correct xp using the signal value (define your correction logic)\n",
    "        corrected_xp = xp +gain * signal_value  # I add the signal!!!!!\n",
    "        # Append the updated particle to the new list\n",
    "        corrected_pts.append([x, corrected_xp, delt, Tau])\n",
    "      return corrected_pts\n",
    "\n",
    "def kickL(pts, signalL,gainL):\n",
    "     corrected_pts = []\n",
    "     for x, xp, delt, Tau in pts:\n",
    "        # Interpolate signalL at the particle's Tau\n",
    "        signal_value = signalL(Tau)\n",
    "        # Correct xp using the signal value (define your correction logic)\n",
    "        corrected_delt = delt + gainL * signal_value  # Example: subtracting scaled signal\n",
    "        # Append the upd1.0pend([x, xp, corrected_delt, Tau])\n",
    "        corrected_pts.append([x, xp, corrected_delt, Tau])\n",
    "     return corrected_pts\n",
    "\n",
    "respH_new_once = respH(np.arange(wlH, wrH, DTSamp/over_sample_factor ))\n",
    "respL_new_once = respL(np.arange(wlL, wrL, DTSamp/over_sample_factor ))\n",
    "\n",
    "def Cooling(gain,gainL,Nparts,Nturns):\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "# definition of the transverse \"response function\", which is given by the \n",
    "#    routine resp for the interval -wlH < Dt < wrH and 0 outside\n",
    "    \n",
    "#rng = np.random.default_rng(18022005)\n",
    "    #rng = np.random.default_rng(2275405 )   #this gives us the random seed!!!!\n",
    "\n",
    "# Total width (particles of both colors) of beam time window is 2*Th ('h' for half)\n",
    "#  for sampling each sub-window (red or blue) divided into NSamp intervals. This\n",
    "#  means samp,ing rate DTsamp and NSamp + 1 points. On both ends to be added \n",
    "#  ceiling( Tovlap/DTSamp ) points   \n",
    "    \n",
    "\n",
    "    print('========>', NSamp, NSampext, DTSamp, Th, (wrH - wlH)/DTSamp)\n",
    "\n",
    "    \n",
    "# Routine to generate time traces\n",
    "\n",
    "   \n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "# Computation of transverse transfer matrix elements\n",
    "    projPU11 =  betPU**(1/2)\n",
    "    projPU21 = -alfPU/betPU**(1/2)\n",
    "    projPU22 =  1/betPU**(1/2)\n",
    "\n",
    "    matPUK11 = ((betK/betPU)**(1/2))*(np.cos(muPUK) + alfPU*np.sin(muPUK))\n",
    "    matPUK12 = ((betPU*betK)**(1/2))*np.sin(muPUK)\n",
    "    matPUK21 = ((alfPU-alfK)*np.cos(muPUK) - (1+alfPU*alfK)*np.sin(muPUK))/((betPU*betK)**(1/2))\n",
    "    matPUK22 = ((betPU/betK)**(1/2))*(np.cos(muPUK) - alfK*np.sin(muPUK))\n",
    "\n",
    "    matKPU11 = ((betPU/betK)**(1/2))*(np.cos(muKPU) + alfK*np.sin(muKPU))\n",
    "    matKPU12 = ((betPU*betK)**(1/2))*np.sin(muKPU)\n",
    "    matKPU21 = ((alfK-alfPU)*np.cos(muKPU) - (1+alfPU*alfK)*np.sin(muKPU))/((betPU*betK)**(1/2))\n",
    "    matKPU22 = ((betK/betPU)**(1/2))*(np.cos(muKPU) - alfPU*np.sin(muKPU))\n",
    "\n",
    "# Generate macroparticles in two groups with red ones and blue ones\n",
    "#    .. say we start with red particles the early ones\n",
    "    ptsRed  = []\n",
    "    ptsBlue = []\n",
    "    for _ in range(Nparts):\n",
    "      Tau = rng.uniform(-Th, Th)\n",
    "      xn, xpn, deltn = rng.normal(0., 1.), rng.normal(0., 1.), rng.normal(0., 1.)\n",
    "      if Tau < 0: \n",
    "        ptsRed += [ [projPU11*xn, projPU21*xn + projPU22*xpn, \n",
    "                     deltn*sigdelt, Tau + Th/2] ]\n",
    "      else:\n",
    "        ptsBlue += [ [projPU11*xn, projPU21*xn + projPU22*xpn, \n",
    "                      deltn*sigdelt, Tau - Th/2] ]\n",
    "\n",
    "\n",
    "    emittance=np.zeros(Nturns+2)\n",
    "    geo_emittance=np.zeros(Nturns+2)\n",
    "    rms_momentum_spread=np.zeros(Nturns+2)\n",
    "    beam=[]\n",
    "    xp_beam=[]\n",
    "    x_beam=[]\n",
    "    beam=ptsRed+ptsBlue\n",
    "    beam = np.array(beam)\n",
    "    x_beam=beam[:,0]\n",
    "    xp_beam=beam[:,1]\n",
    "    x_initial=x_beam\n",
    "    xp_initial=xp_beam\n",
    "    tau_initial=beam[:,3]\n",
    "    delt_initial=beam[:,2]\n",
    "    counts_initial, bins_initial = np.histogram(delt_initial, bins=30)\n",
    "    emittance[0]=np.sqrt(np.mean(x_initial**2)*np.mean(xp_initial**2)-np.mean(x_initial*xp_initial)**2)   \n",
    "    geo_emittance[0]=((gamPU*np.mean(x_initial**2)+2*alfPU*np.mean(x_initial*xp_initial)+betPU*np.mean(xp_initial**2)))/2\n",
    "#emittance_individual_particles[0][:]=(gamPU*x_beam[:10]**2+2*alfPU*(x_beam[:10]*xp_beam[:10])+betPU*(xp_beam[:10]**2))/2\n",
    "    rms_momentum_spread[0]=np.sqrt(np.mean(delt_initial**2)-np.mean(delt_initial)**2)\n",
    "\n",
    "\n",
    "\n",
    "    ptsRedprev = ptsRed\n",
    "    ptsRed = [ [matPUK11*x + matPUK12*xp, matPUK21*x + matPUK22*xp, delt, \n",
    "               Tau + etaPUK*delt*Trev] for x, xp, delt, Tau in ptsRed ]  # no action at kicker\n",
    "    ptsRed = [ [matKPU11*x + matKPU12*xp, matKPU21*x + matKPU22*xp, delt, \n",
    "               Tau + etaKPU*delt*Trev] for x, xp, delt, Tau in ptsRed]\n",
    "\n",
    "# Prepare for tracking of blue particles and track them once around\n",
    "    ptsBlue, ptsRed = PartsExchange( ptsBlue, ptsRed,Th )\n",
    "    ptsBlueprev = ptsBlue\n",
    "    time2,TrHBlue, TrLBlue = TimeTraces( ptsRedprev, ptsBlue, ptsRed,t_bins,Th )\n",
    "\n",
    "    ptsBlue = [ [matPUK11*x + matPUK12*xp, matPUK21*x + matPUK22*xp, delt, \n",
    "            Tau + etaPUK*delt*Trev] for x, xp, delt, Tau in ptsBlue ]  # no action at kicker\n",
    "#ptsBlue=kickH(ptsBlue,cs)\n",
    "    ptsBlue = [ [matKPU11*x + matKPU12*xp, matKPU21*x + matKPU22*xp, delt, \n",
    "            Tau + etaKPU*delt*Trev] for x, xp, delt, Tau in ptsBlue]\n",
    "    TrHBlue_prev, TrLBlue_prev = TrHBlue, TrLBlue\n",
    "# Prepare for 2nd tracking of red particles and track them once around\n",
    "    ptsRed, ptsBlue = PartsExchange( ptsRed, ptsBlue,Th )\n",
    "    ptsRedprev = ptsRed\n",
    "    time2,TrHRed, TrLRed = TimeTraces( ptsBlueprev, ptsRed, ptsBlue,t_bins,Th )\n",
    "#cs = sp.interpolate.CubicSpline( DTSamp*np.arange(-NSampext, NSamp + 1 + NSampext)-Th/2, \n",
    " #                 TrHRed[-NSampext:] + TrHRed[:NSamp + 1 + NSampext] )\n",
    "    ptsRed = [ [matPUK11*x + matPUK12*xp, matPUK21*x + matPUK22*xp, delt, \n",
    "            Tau + etaPUK*delt*Trev] for x, xp, delt, Tau in ptsRed ]  # no action at kicker\n",
    "#ptsRed=kickH(ptsRed,cs)\n",
    "    ptsRed = [ [matKPU11*x + matKPU12*xp, matKPU21*x + matKPU22*xp, delt, \n",
    "            Tau + etaKPU*delt*Trev] for x, xp, delt, Tau in ptsRed]\n",
    "    TrHRed_prev, TrLRed_prev = TrHRed, TrLRed\n",
    "# Now the blue ones are early - now just enter loop over number of turns\n",
    "#   later - for longitudinal filter cooling - some more tracking will be needed\n",
    "\n",
    "    beam=[]\n",
    "    xp_beam=[]\n",
    "    x_beam=[]\n",
    "    beam=ptsRed+ptsBlue\n",
    "    beam = np.array(beam)\n",
    "    x_beam=beam[:,0]\n",
    "    xp_beam=beam[:,1]\n",
    "    dtl_beam=beam[:,2]\n",
    "    emittance[1]=np.sqrt(np.mean(x_beam**2)*np.mean(xp_beam**2)-np.mean(x_beam*xp_beam)**2) \n",
    "#emittance_individual_particles[1][:]=(gamPU*x_beam[:10]**2+2*alfPU*(x_beam[:10]*xp_beam[:10])+betPU*(xp_beam[:10]**2))/2\n",
    "    rms_momentum_spread[1]=np.sqrt(np.mean(dtl_beam**2)-np.mean(dtl_beam)**2)\n",
    "\n",
    "# Now we can go in loop and even implement transverse cooling or longitudinal time delay cooling (but not filter)\n",
    "    for turn in range(Nturns):\n",
    "    #print(f' turn {turn:5d} after {(time.time() - datin):8.2f} s <===')\n",
    "#   Get ready for the blue particles to be tracked and track them to kicker\n",
    "     ptsBlue, ptsRed = PartsExchange( ptsBlue, ptsRed,Th )\n",
    "     ptsBlueprev, TrLBlueprev = ptsBlue, TrLBlue  # Keep longitudinal trace to add filter cooling later\n",
    "     time2,TrHBlue, TrLBlue = TimeTraces( ptsRedprev, ptsBlue, ptsRed,t_bins,Th )\n",
    "     TrLDiffBlue = [h - l for h, l in zip(TrLBlue_prev, TrLBlue)]\n",
    "     cs = sp.interpolate.CubicSpline( time2, \n",
    "                  TrHBlue )\n",
    "     csL=sp.interpolate.CubicSpline( time2, \n",
    "                  np.array(TrLDiffBlue) )\n",
    "    #cs = sp.interpolate.interp1d( DTSamp*np.arange(-NSampext, NSamp + 1 + NSampext)-Th/2, \n",
    "     #             TrHBlue[-NSampext:] + TrHBlue[:NSamp + 1 + NSampext] )\n",
    "    #csL=sp.interpolate.interp1d( DTSamp*np.arange(-NSampext, NSamp + 1 + NSampext)-Th/2, \n",
    "     #             TrLBlue[-NSampext:] + TrLBlue[:NSamp + 1 + NSampext] )\n",
    "\n",
    "    #print(len(TrHBlue))\n",
    "     ptsBlue = [ [matPUK11*x + matPUK12*xp, matPUK21*x + matPUK22*xp, delt, \n",
    "                 Tau + etaPUK*delt*Trev] for x, xp, delt, Tau in ptsBlue ]\n",
    "#   One day cooling to be applied -  now just dump time traces instead\n",
    "     ptsBlue=kickH(ptsBlue,cs,gain)\n",
    "     ptsBlue=kickL(ptsBlue,csL,gainL)\n",
    "#   track the blue one further to the PU\n",
    "     ptsBlue = [ [matKPU11*x + matKPU12*xp, matKPU21*x + matKPU22*xp, delt, \n",
    "                 Tau + etaKPU*delt*Trev] for x, xp, delt, Tau in ptsBlue ]\n",
    "     TrHBlue_prev, TrLBlue_prev = TrHBlue, TrLBlue\n",
    "#   Get ready for the red particles to be tracked and track them to kicker\n",
    "     ptsRed, ptsBlue = PartsExchange( ptsRed, ptsBlue,Th )\n",
    "     ptsRedprev, TrLRedprev = ptsRed, TrLRed  # Keep longitudinal trace to add filter cooling later\n",
    "     time2,TrHRed, TrLRed = TimeTraces( ptsBlueprev, ptsRed, ptsBlue,t_bins,Th )\n",
    "     TrLDiffRed = [h - l for h, l in zip(TrLRed_prev, TrLRed)]\n",
    "     cs = sp.interpolate.CubicSpline( time2, \n",
    "                  TrHRed )\n",
    "     csL=sp.interpolate.CubicSpline( time2, \n",
    "                  np.array(TrLDiffRed) )\n",
    "    \n",
    "     y_cs = cs(time2)\n",
    "     y_csL = csL(time2)\n",
    "    #print(\"Spline Knots:\", cs.x)  # Knots where the spline pieces join\n",
    "    #print(\"Spline Coefficients:\", cs.c)  # Coefficients of the spline pieces\n",
    "   \n",
    "     tauPU=[]\n",
    "     xPU=[]\n",
    "     beam=ptsRed\n",
    "     beam = np.array(beam)\n",
    "     tauPU=beam[:,3]\n",
    "     xPU=beam[:,0]\n",
    "    \n",
    "    \n",
    "# Plot the spline\n",
    "     '''\n",
    "     if turn==Nturns-1:\n",
    "      plt.figure(figsize=(8, 5))\n",
    "      plt.plot(time2, y_cs, label=\"TrHRed Interpolated\", linestyle=\"-\")\n",
    "     #plt.plot(t_smooth, y_csL, label=\"TrLDiffRed Interpolated\", linestyle=\"--\")\n",
    "      plt.scatter(time2, TrHRed, marker=\"o\", color=\"blue\", label=\"horizontal signal\")\n",
    "      plt.scatter(tauPU, xPU, zorder=5, color=\"red\", label=\"position of particles\")\n",
    "     '''\n",
    "     \n",
    "     ptsRed = [ [matPUK11*x + matPUK12*xp, matPUK21*x + matPUK22*xp, delt, \n",
    "                Tau + etaPUK*delt*Trev] for x, xp, delt, Tau in ptsRed ]\n",
    "#   One day cooling(s) to be applied - now just dump time traces instead\n",
    "     ptsRed=kickH(ptsRed,cs,gain)\n",
    "     ptsRed=kickL(ptsRed,csL,gainL)   #time of flight method longitudinal cooling\n",
    "#   track the red ones further to the PU, where they will be the late ones\n",
    "     ptsRed = [ [matKPU11*x + matKPU12*xp, matKPU21*x + matKPU22*xp, delt, \n",
    "                Tau + etaKPU*delt*Trev] for x, xp, delt, Tau in ptsRed ] \n",
    "     TrHRed_prev, TrLRed_prev = TrHRed, TrLRed\n",
    "     beam=[]\n",
    "     xp_beam=[]\n",
    "     x_beam=[]\n",
    "     delta_beam=[]\n",
    "     dlt_final=[]\n",
    "     tau_final=[]\n",
    "     beam=ptsRed+ptsBlue\n",
    "     beam = np.array(beam)\n",
    "     x_beam=beam[:,0]\n",
    "     xp_beam=beam[:,1]\n",
    "     delta_beam=beam[:,2]\n",
    "     tau_final=beam[:,3]\n",
    "     dlt_final=beam[:,2]\n",
    "     counts_final, bins_final = np.histogram(dlt_final, bins=30)\n",
    "     emittance[turn+2]=np.sqrt(np.mean(x_beam**2)*np.mean(xp_beam**2)-np.mean(x_beam*xp_beam)**2)\n",
    "     geo_emittance[turn+2]=(gamPU*np.mean(x_beam**2)+2*alfPU*np.mean(x_beam*xp_beam)+betPU*np.mean(xp_beam**2))/2\n",
    "     rms_momentum_spread[turn+2]=np.sqrt(np.mean(delta_beam**2)-np.mean(delta_beam)**2)\n",
    "    #emittance_individual_particles[turn+2][:]=(gamPU*x_beam[:10]**2+2*alfPU*(x_beam[:10]*xp_beam[:10])+betPU*(xp_beam[:10]**2))/2\n",
    "\n",
    "    print(f'The initial emittance is {emittance[0]}{chr(0x03C0)} mm*mrad')\n",
    "    print(f'The final emittance after cooling is {emittance[-1]}{chr(0x03C0)} mm*mrad')\n",
    "    print(f'Is emittance reduced? -->  {emittance[-1] < emittance[0]}')\n",
    "    if emittance[-1] < emittance[0]:\n",
    "      print(f\"Emittance is reduced by {(emittance[0] - emittance[-1]) * 100 / emittance[0]}%\")\n",
    "    print(f'The initial rms_momentum_spread is {rms_momentum_spread[0]} ')\n",
    "    print(f'The final rms_momentum_spread after cooling is {rms_momentum_spread[-1]} ')\n",
    "    print(f'Is rms_momentum_spread reduced? -->  {rms_momentum_spread[-1] < rms_momentum_spread[0]}')\n",
    "    if rms_momentum_spread[-1] < rms_momentum_spread[0]:\n",
    "      print(f\"rms_momentum_spread is reduced by {(rms_momentum_spread[0] - rms_momentum_spread[-1]) * 100 / rms_momentum_spread[0]}%\")\n",
    "\n",
    "    return emittance[-1], rms_momentum_spread[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========> 800 1 1.2499999999999998e-10 1e-07 25.600000000000005\n",
      "The initial emittance is 0.9785205504150565π mm*mrad\n",
      "The final emittance after cooling is 0.050902226014428925π mm*mrad\n",
      "Is emittance reduced? -->  True\n",
      "Emittance is reduced by 94.79804220843008%\n",
      "The initial rms_momentum_spread is 9.920147303663395e-05 \n",
      "The final rms_momentum_spread after cooling is 1.4873687125500405e-05 \n",
      "Is rms_momentum_spread reduced? -->  True\n",
      "rms_momentum_spread is reduced by 85.00658642436919%\n",
      "Peak memory usage: 258.30 MB\n"
     ]
    }
   ],
   "source": [
    "#3000 particles 20000 turns\n",
    "\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "mem_usage = memory_usage(lambda :Cooling(0.0002,2e-8,3000,20000), interval=0.1)\n",
    "print(f\"Peak memory usage: {max(mem_usage):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========> 800 1 1.2499999999999998e-10 1e-07 25.600000000000005\n",
      "The initial emittance is 1.0015662118483675π mm*mrad\n",
      "The final emittance after cooling is 1837.7873351912053π mm*mrad\n",
      "Is emittance reduced? -->  False\n",
      "The initial rms_momentum_spread is 0.00010005445134838862 \n",
      "The final rms_momentum_spread after cooling is 1.5550493599113113e-05 \n",
      "Is rms_momentum_spread reduced? -->  True\n",
      "rms_momentum_spread is reduced by 84.45796924619931%\n",
      "Peak memory usage: 262.68 MB\n"
     ]
    }
   ],
   "source": [
    "#8000 particles 20000 turns\n",
    "\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "mem_usage = memory_usage(lambda :Cooling(0.0002,2e-8,8000,20000), interval=0.1)\n",
    "print(f\"Peak memory usage: {max(mem_usage):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
